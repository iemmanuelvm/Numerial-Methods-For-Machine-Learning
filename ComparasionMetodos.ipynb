{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComparaciÃ³n de mÃ©todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las modulos\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import time\n",
    "from numpy import linalg as LA\n",
    "import random\n",
    "from time import time\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la funcion a minimizar\n",
    "def f(x):\n",
    "    fx = 100*(np.sqrt(x[0]**2+(x[1]+1)**2)-1)**2 + 90*(np.sqrt(x[0]**2+(x[1]+1)**2)-1)**2 -(20*x[0]+40*x[1])\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir funcion para calcular la matriz del gradiente\n",
    "def gradient(x,delta):\n",
    "    grad=np.zeros(2)\n",
    "    grad[0]=(f([x[0]+delta,x[1]])- f([x[0]-delta,x[1]]))/(2*delta)\n",
    "    grad[1]=(f([x[0],x[1]+delta])- f([x[0],x[1]-delta]))/(2*delta)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir funcion para calcular la matriz Hessiana\n",
    "def Hessian(x,delta):\n",
    "    H=np.zeros([2,2])\n",
    "    H[0,0]= (f([x[0]+delta,x[1]]) - 2*f([x[0],x[1]]) +  f([x[0]-delta,x[1]]))/(delta**2)\n",
    "    H[1,1]= (f([x[0],x[1]+delta]) - 2*f([x[0],x[1]]) +  f([x[0],x[1]-delta]))/(delta**2)\n",
    "    H[0,1]= (f([x[0]+delta,x[1]+delta]) - f([x[0]+delta,x[1]-delta]) - f([x[0]-delta,x[1]+delta]) +  f([x[0]-delta,x[1]-delta]))/(4*delta**2)\n",
    "    H[1,0]= H[0,1]\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos el algoritmo de golden section para encontrar alpha optimo\n",
    "def golden(x,search,xi,eps):\n",
    "    a = xi[0];\n",
    "    b = xi[1];\n",
    "    tau = 0.381967;\n",
    "    alpha1 = a*(1-tau) + b*tau;\n",
    "    alpha2 = a*tau + b*(1-tau);\n",
    "    falpha1 = f(x+alpha1*search);\n",
    "    falpha2 = f(x+alpha2*search);\n",
    "    for i in range(100):\n",
    "        if falpha1 > falpha2:\n",
    "            a = alpha1;\n",
    "            alpha1 = alpha2;\n",
    "            falpha1 = falpha2;\n",
    "            alpha2 = tau*a + (1-tau)*b;\n",
    "            falpha2 = f(x+alpha2*search);\n",
    "        else:\n",
    "            b = alpha2;\n",
    "            alpha2 = alpha1;\n",
    "            falpha2 = falpha1;\n",
    "            alpha1 = tau*b + (1-tau)*a;\n",
    "            falpha1 = f(x+alpha1*search);\n",
    "        if np.abs(f(x+alpha1*search)- f(x+alpha2*search)) < eps :\n",
    "            break;\n",
    "    return alpha1,falpha1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos la funcion de prueba\n",
    "X1 = np.arange(-5, 5, 0.01)\n",
    "X2 = np.arange(-5, 5, 0.01)\n",
    "x1, x2 = np.meshgrid(X1, X2)\n",
    "z = f([x1,x2])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection = '3d')\n",
    "surf = ax.plot_surface(x1, x2, z, cmap=cm.autumn, linewidth=0, antialiased = False, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable en comun de los mÃ©todos\n",
    "delta = 1e-3 \n",
    "ep1 = 1e-3\n",
    "puntosFB = []\n",
    "puntosSD = []\n",
    "puntosN = []\n",
    "puntosM = []\n",
    "puntosGC = []\n",
    "puntosBFP = []\n",
    "puntosBFGS = []\n",
    "times = []\n",
    "iters = []\n",
    "pmins = []\n",
    "xoptm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficaUnaaUna(puntos, nombreMetodo):\n",
    "    #Generamos valores ramdom para generar un color diferente de grafica\n",
    "    r = random.random()\n",
    "    b = random.random()\n",
    "    g = random.random()\n",
    "    color = (r, g, b)\n",
    "    aux = 0\n",
    "    x = np.linspace(-1.3, 1.3, 100)\n",
    "    y = np.linspace(-3, 1.3, 100)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = f([xx, yy])\n",
    "    plt.contour(x,y,z,20)\n",
    "    plt.plot(-1,1, 'ro--',  markersize=6)\n",
    "    plt.plot(0.5,0, 'go--', markersize=6)\n",
    "    #Graficamos los trazos (direcciones) que va generando el algoritmo\n",
    "    for pts in puntos:\n",
    "        if(aux==len(puntos)-1):\n",
    "            plt.plot(pts[0],pts[1], c=color, linewidth=2, label=str(nombreMetodo))\n",
    "        else:\n",
    "            plt.plot(pts[0],pts[1], c=color, linewidth=2)\n",
    "        aux = aux + 1\n",
    "    plt.legend()    \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la funciÃ³n inicial = 270.294169 \n",
      "0\t[-0.7699,0.6199]\t\t110.2512\t\t444.3158\n",
      "1\t[-0.6205,0.3875]\t\t48.2750\t\t276.2647\n",
      "2\t[-0.5198,0.2472]\t\t23.9401\t\t172.7293\n",
      "3\t[-0.4485,0.1640]\t\t14.0401\t\t109.5750\n",
      "4\t[-0.3947,0.1163]\t\t9.6750\t\t71.9210\n",
      "5\t[-0.3514,0.0903]\t\t7.4400\t\t50.4692\n",
      "6\t[-0.3144,0.0777]\t\t6.0375\t\t39.0719\n",
      "7\t[-0.2814,0.0730]\t\t4.9762\t\t33.3881\n",
      "8\t[-0.2508,0.0728]\t\t4.0712\t\t30.5303\n",
      "9\t[-0.2220,0.0752]\t\t3.2533\t\t28.8984\n",
      "10\t[-0.1945,0.0787]\t\t2.4967\t\t27.7526\n",
      "11\t[-0.1680,0.0828]\t\t1.7909\t\t26.7903\n",
      "12\t[-0.1425,0.0868]\t\t1.1314\t\t25.8974\n",
      "13\t[-0.1177,0.0906]\t\t0.5152\t\t25.0343\n",
      "14\t[-0.0937,0.0940]\t\t-0.0599\t\t24.1892\n",
      "15\t[-0.0706,0.0969]\t\t-0.5961\t\t23.3599\n",
      "16\t[-0.0482,0.0993]\t\t-1.0955\t\t22.5472\n",
      "17\t[-0.0265,0.1012]\t\t-1.5602\t\t21.7524\n",
      "18\t[-0.0056,0.1026]\t\t-1.9923\t\t20.9767\n",
      "19\t[0.0146,0.1036]\t\t-2.3938\t\t20.2211\n",
      "20\t[0.0341,0.1042]\t\t-2.7665\t\t19.4860\n",
      "21\t[0.0529,0.1044]\t\t-3.1124\t\t18.7720\n",
      "22\t[0.0710,0.1043]\t\t-3.4332\t\t18.0792\n",
      "23\t[0.0884,0.1039]\t\t-3.7305\t\t17.4074\n",
      "24\t[0.1051,0.1032]\t\t-4.0060\t\t16.7568\n",
      "25\t[0.1212,0.1023]\t\t-4.2612\t\t16.1270\n",
      "26\t[0.1367,0.1011]\t\t-4.4974\t\t15.5179\n",
      "27\t[0.1516,0.0998]\t\t-4.7161\t\t14.9290\n",
      "28\t[0.1658,0.0983]\t\t-4.9183\t\t14.3601\n",
      "29\t[0.1796,0.0967]\t\t-5.1054\t\t13.8108\n",
      "30\t[0.1927,0.0950]\t\t-5.2784\t\t13.2806\n",
      "31\t[0.2054,0.0931]\t\t-5.4383\t\t12.7690\n",
      "32\t[0.2175,0.0912]\t\t-5.5861\t\t12.2757\n",
      "33\t[0.2291,0.0892]\t\t-5.7226\t\t11.8002\n",
      "34\t[0.2403,0.0872]\t\t-5.8487\t\t11.3418\n",
      "35\t[0.2510,0.0851]\t\t-5.9652\t\t10.9003\n",
      "36\t[0.2612,0.0830]\t\t-6.0728\t\t10.4750\n",
      "37\t[0.2711,0.0809]\t\t-6.1721\t\t10.0655\n",
      "38\t[0.2805,0.0787]\t\t-6.2638\t\t9.6713\n",
      "39\t[0.2895,0.0766]\t\t-6.3485\t\t9.2919\n",
      "40\t[0.2982,0.0744]\t\t-6.4266\t\t8.9268\n",
      "41\t[0.3065,0.0723]\t\t-6.4987\t\t8.5755\n",
      "42\t[0.3145,0.0702]\t\t-6.5652\t\t8.2376\n",
      "43\t[0.3221,0.0681]\t\t-6.6266\t\t7.9127\n",
      "44\t[0.3294,0.0660]\t\t-6.6832\t\t7.6002\n",
      "45\t[0.3364,0.0640]\t\t-6.7354\t\t7.2997\n",
      "46\t[0.3432,0.0620]\t\t-6.7836\t\t7.0109\n",
      "47\t[0.3496,0.0600]\t\t-6.8281\t\t6.7332\n",
      "48\t[0.3558,0.0581]\t\t-6.8690\t\t6.4663\n",
      "49\t[0.3617,0.0562]\t\t-6.9068\t\t6.2098\n",
      "50\t[0.3673,0.0544]\t\t-6.9417\t\t5.9634\n",
      "51\t[0.3728,0.0525]\t\t-6.9738\t\t5.7265\n",
      "52\t[0.3780,0.0508]\t\t-7.0035\t\t5.4989\n",
      "53\t[0.3830,0.0490]\t\t-7.0308\t\t5.2803\n",
      "54\t[0.3878,0.0474]\t\t-7.0560\t\t5.0702\n",
      "55\t[0.3923,0.0457]\t\t-7.0792\t\t4.8684\n",
      "56\t[0.3967,0.0441]\t\t-7.1006\t\t4.6746\n",
      "57\t[0.4010,0.0426]\t\t-7.1204\t\t4.4884\n",
      "58\t[0.4050,0.0411]\t\t-7.1386\t\t4.3095\n",
      "59\t[0.4089,0.0396]\t\t-7.1554\t\t4.1378\n",
      "60\t[0.4126,0.0382]\t\t-7.1708\t\t3.9728\n",
      "61\t[0.4161,0.0368]\t\t-7.1851\t\t3.8143\n",
      "62\t[0.4195,0.0355]\t\t-7.1982\t\t3.6622\n",
      "63\t[0.4228,0.0342]\t\t-7.2104\t\t3.5160\n",
      "64\t[0.4259,0.0329]\t\t-7.2215\t\t3.3757\n",
      "65\t[0.4289,0.0317]\t\t-7.2318\t\t3.2410\n",
      "66\t[0.4318,0.0306]\t\t-7.2413\t\t3.1116\n",
      "67\t[0.4346,0.0294]\t\t-7.2501\t\t2.9873\n",
      "68\t[0.4372,0.0283]\t\t-7.2581\t\t2.8680\n",
      "69\t[0.4398,0.0273]\t\t-7.2656\t\t2.7534\n",
      "70\t[0.4422,0.0262]\t\t-7.2724\t\t2.6434\n",
      "71\t[0.4446,0.0252]\t\t-7.2787\t\t2.5378\n",
      "72\t[0.4468,0.0243]\t\t-7.2845\t\t2.4364\n",
      "73\t[0.4489,0.0234]\t\t-7.2899\t\t2.3390\n",
      "74\t[0.4510,0.0225]\t\t-7.2948\t\t2.2456\n",
      "75\t[0.4530,0.0216]\t\t-7.2994\t\t2.1558\n",
      "76\t[0.4549,0.0208]\t\t-7.3036\t\t2.0696\n",
      "77\t[0.4567,0.0200]\t\t-7.3075\t\t1.9869\n",
      "78\t[0.4584,0.0192]\t\t-7.3110\t\t1.9075\n",
      "79\t[0.4601,0.0185]\t\t-7.3143\t\t1.8312\n",
      "80\t[0.4617,0.0177]\t\t-7.3173\t\t1.7580\n",
      "81\t[0.4633,0.0171]\t\t-7.3201\t\t1.6877\n",
      "82\t[0.4647,0.0164]\t\t-7.3227\t\t1.6202\n",
      "83\t[0.4662,0.0158]\t\t-7.3251\t\t1.5554\n",
      "84\t[0.4675,0.0151]\t\t-7.3273\t\t1.4932\n",
      "85\t[0.4688,0.0145]\t\t-7.3293\t\t1.4335\n",
      "86\t[0.4701,0.0140]\t\t-7.3311\t\t1.3761\n",
      "87\t[0.4713,0.0134]\t\t-7.3328\t\t1.3211\n",
      "88\t[0.4724,0.0129]\t\t-7.3344\t\t1.2683\n",
      "89\t[0.4735,0.0124]\t\t-7.3359\t\t1.2175\n",
      "90\t[0.4746,0.0119]\t\t-7.3372\t\t1.1688\n",
      "91\t[0.4756,0.0114]\t\t-7.3384\t\t1.1221\n",
      "92\t[0.4766,0.0109]\t\t-7.3396\t\t1.0772\n",
      "93\t[0.4775,0.0105]\t\t-7.3406\t\t1.0341\n",
      "94\t[0.4775,0.0105]\t\t-7.3406\t\t0.9927\n",
      "Tiempo transcurrido: 6.977081\n",
      "Iteraciones: 95\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "xi_fb = [-1,1] #Punto de partida\n",
    "xa_fb =  xi_fb\n",
    "alpha = 0.001\n",
    "iteraciones = 1\n",
    "fx_value = f(xi_fb) #Se evalua la funcion con el punto inicial\n",
    "print('Valor de la funciÃ³n inicial = %f ' % fx_value)\n",
    "for i in range(1000): #Proceso iterativo del algoritmo\n",
    "    xa_fb =  xi_fb\n",
    "    si = gradient(xi_fb, delta)\n",
    "    s1 = -1*si[0]\n",
    "    s2 = -1*si[1]\n",
    "    xdd = xi_fb[0] + alpha*s1\n",
    "    ydd = xi_fb[1] + alpha*s2\n",
    "    fdd1 = f([xdd, ydd])\n",
    "    fdd2 = f(xi_fb)\n",
    "    if(abs(fdd1-fdd2)<=ep1):\n",
    "        print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, xi_fb[0], xi_fb[1], f(xi_fb), LA.norm(si)))\n",
    "        break\n",
    "    xi_fb = [xdd, ydd]\n",
    "    print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, xi_fb[0], xi_fb[1], f(xi_fb), LA.norm(si)))\n",
    "    puntosFB.append([[xa_fb[0],xi_fb[0]],[xa_fb[1],xi_fb[1]]]) #Almacenamos los punto de direccion del algoritmo\n",
    "    iteraciones += 1\n",
    "elapsed_time = (time() - start_time)*1000\n",
    "print(\"Tiempo transcurrido: %f\" % elapsed_time)\n",
    "print(\"Iteraciones: %d\" % iteraciones)\n",
    "#Graficamos el resultado del Steepest descent method\n",
    "graficaUnaaUna(puntosFB, 'Brute Force')\n",
    "times.append(elapsed_time)\n",
    "iters.append(iteraciones)\n",
    "pmins.append(f(xi_fb))\n",
    "xoptm.append(xi_fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la funciÃ³n inicial = 270.294169 \n",
      "0\t[-0.3892,-0.0092]\t\t8.9425\t\t444.3158\n",
      "1\t[-0.0345,0.2013]\t\t0.3751\t\t33.6772\n",
      "2\t[0.0410,0.0766]\t\t-2.7472\t\t42.8521\n",
      "3\t[0.1616,0.1444]\t\t-4.3989\t\t21.6597\n",
      "4\t[0.2127,0.0633]\t\t-5.4337\t\t21.9813\n",
      "5\t[0.2783,0.1043]\t\t-6.0764\t\t16.1606\n",
      "6\t[0.3148,0.0471]\t\t-6.5224\t\t13.2170\n",
      "7\t[0.3540,0.0712]\t\t-6.8064\t\t11.4880\n",
      "8\t[0.3868,0.0277]\t\t-7.0162\t\t7.8351\n",
      "9\t[0.4105,0.0454]\t\t-7.1464\t\t8.5604\n",
      "10\t[0.4349,0.0157]\t\t-7.2356\t\t4.5741\n",
      "11\t[0.4483,0.0261]\t\t-7.2851\t\t5.4709\n",
      "12\t[0.4691,0.0061]\t\t-7.3234\t\t2.4656\n",
      "13\t[0.4763,0.0126]\t\t-7.3382\t\t3.1097\n",
      "14\t[0.4833,0.0040]\t\t-7.3451\t\t1.1885\n",
      "15\t[0.4869,0.0067]\t\t-7.3485\t\t1.4562\n",
      "16\t[0.4926,0.0006]\t\t-7.3509\t\t0.6321\n",
      "17\t[0.4930,0.0011]\t\t-7.3509\t\t0.9605\n",
      "Tiempo transcurrido: 8.971453\n",
      "Iteraciones: 18\n"
     ]
    }
   ],
   "source": [
    "start_time = time() #Conteo para tiempo\n",
    "iteraciones = 1 #Conteo de numero de iteraciones\n",
    "xi_sd = [-1,1] #Punto de partida\n",
    "xi_1 = [-1,1] #Punto inicial para el metodo del golden section\n",
    "fx_value = f(xi_sd) #Se evalua la funcion con el punto inicial\n",
    "print('Valor de la funciÃ³n inicial = %f ' % fx_value)\n",
    "for i in range(1000): #Proceso iterativo del algoritmo\n",
    "    grad = gradient(xi_1,delta) #Se obtiene el vector del gradiente de la funcion objetivo\n",
    "    si = -grad #La direccion de busqueda es el gradiente negativo\n",
    "    alpha, fx_curr = golden(xi_1,si,xi_sd,ep1) #Se encuentra el alpha optimo (tamaÃ±o de paso)\n",
    "    x_opt = xi_1 + alpha*si #Se encuentra el nuevo optimo\n",
    "    if abs(fx_curr-fx_value)<ep1 or LA.norm(grad)<ep1: #Establecemos como condicion de paro, que la diferencia fx_anterior-fx_actual sea menor a epsilon o que la norma del gradiente sea menor a epsilon\n",
    "        print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, x_opt[0], x_opt[1],fx_value,LA.norm(grad)))\n",
    "        break\n",
    "    fx_value=fx_curr #Ahora fx_value se iguala con el actual, de esta forma el actual ahora es el anterior\n",
    "    print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, x_opt[0], x_opt[1],fx_value,LA.norm(grad)))\n",
    "    puntosSD.append([[xi_1[0],x_opt[0]],[xi_1[1],x_opt[1]]]) #Almacenamos los punto de direccion del algoritmo\n",
    "    xi_1=x_opt #ahora x anterior es x_opt\n",
    "    iteraciones += 1\n",
    "elapsed_time = (time() - start_time)*1000\n",
    "print(\"Tiempo transcurrido: %f\" % elapsed_time)\n",
    "print(\"Iteraciones: %d\" % iteraciones)\n",
    "#Graficamos el resultado del Steepest descent method\n",
    "graficaUnaaUna(puntosSD, 'Steepest descent method')\n",
    "times.append(elapsed_time)\n",
    "iters.append(iteraciones)\n",
    "pmins.append(f(x_opt))\n",
    "xoptm.append(x_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Newtonâ€™s method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la funciÃ³n inicial = 270.294169 \n",
      "0\t[-1.0000,1.0000]\t\t6.5243\t\t444.3158\n",
      "1\t[-0.3266,0.0340]\t\t-3.2287\t\t31.1194\n",
      "2\t[0.1506,0.1848]\t\t-6.8860\t\t34.9350\n",
      "3\t[0.3772,0.0722]\t\t-7.3383\t\t9.3801\n",
      "4\t[0.4844,0.0138]\t\t-7.3515\t\t2.3801\n",
      "5\t[0.4948,0.0042]\t\t-7.3521\t\t0.7266\n",
      "Tiempo transcurrido: 3.987074\n",
      "Iteraciones: 6\n"
     ]
    }
   ],
   "source": [
    "start_time = time() #Conteo para tiempo\n",
    "iteraciones = 1 #Conteo de numero de iteraciones\n",
    "xi = [-1,1] #Punto de partida\n",
    "x = xi #Punto inicial para el metodo de golden section\n",
    "xAnt = xi #Definimos una variable de punto anterior\n",
    "fx_prev = f(x) #Se evalua la funcion con el punto inicial\n",
    "print('Valor de la funciÃ³n inicial = %f ' % fx_prev)\n",
    "for i in range(1000): #Proceso iterativo del algoritmo\n",
    "    xAnt = x #Almacenamos el punto anterior\n",
    "    dire = gradient(x,delta) #Se obtiene la matriz del gradiente de la funcion objetivo\n",
    "    H = Hessian(x,delta) #Se calcula la matriz Hessiana\n",
    "    dire = np.atleast_2d(dire) #La funciÃ³n se usa cuando queremos convertir entradas en matrices con al menos dos dimensiones. Las entradas escalares y unidimensionales se convierten en matrices bidimensionales, mientras que las entradas de mayor dimensiÃ³n se conservan.\n",
    "    si=np.matmul(-LA.inv(H),dire.transpose()) #Se multiplica la inversa de la matriz Hessina por el vector gradiente (de la direccion)\n",
    "    si = si.transpose() #Ahora la nueva direccion de busqueda es la transpuesta esto porque la multiplicacion\n",
    "    si = np.ndarray.flatten(si) #Devuelve una copia de la matriz colapsada en una dimensiÃ³n.\n",
    "    alpha, fx_curr = golden(x,si,xi,ep1) #Se encuentra el alpha optimo (tamaÃ±o de paso)\n",
    "    if abs(fx_curr-fx_prev)<ep1 or LA.norm(dire)<ep1: #Establecemos como condicion de paro, que la diferencia fx_anterior-fx_actual sea menor a epsilon o que la norma del gradiente sea menor a epsilon\n",
    "        print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, x[0], x[1],fx_curr,LA.norm(dire)))\n",
    "        break;\n",
    "    print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, x[0], x[1],fx_curr,LA.norm(dire)))\n",
    "    fx_prev=fx_curr\n",
    "    x = x +  alpha*si #ahora x anterior es x_opt\n",
    "    puntosN.append([[xAnt[0],x[0]],[xAnt[1],x[1]]])\n",
    "    iteraciones += 1\n",
    "elapsed_time = (time() - start_time)*1000\n",
    "print(\"Tiempo transcurrido: %f\" % elapsed_time)\n",
    "print(\"Iteraciones: %d\" % iteraciones)\n",
    "#Graficamos el resultado del Steepest descent method\n",
    "graficaUnaaUna(puntosN, 'Modified Newtonâ€™s method') \n",
    "times.append(elapsed_time)\n",
    "iters.append(iteraciones)\n",
    "pmins.append(f(x))\n",
    "xoptm.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marquardtâ€™s method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la funciÃ³n inicial = 270.294169 \n",
      "0\t[-0.9778,0.9634]\t\t270.2942\t\t444.3158\n",
      "1\t[-0.9363,0.8955]\t\t251.6203\t\t428.0979\n",
      "2\t[-0.8635,0.7779]\t\t218.7471\t\t397.9601\n",
      "3\t[-0.7492,0.5990]\t\t167.3425\t\t345.6663\n",
      "4\t[-0.5978,0.3806]\t\t102.4403\t\t265.5825\n",
      "5\t[-0.4329,0.1956]\t\t45.0861\t\t166.5017\n",
      "6\t[-0.2585,0.1152]\t\t14.8448\t\t79.3220\n",
      "7\t[-0.0282,0.1225]\t\t4.5436\t\t35.1505\n",
      "8\t[0.2325,0.1119]\t\t-1.4682\t\t22.1993\n",
      "9\t[0.3997,0.0554]\t\t-5.6152\t\t14.1528\n",
      "10\t[0.4757,0.0147]\t\t-7.0701\t\t6.2961\n",
      "11\t[0.4968,0.0015]\t\t-7.3353\t\t1.6001\n",
      "12\t[0.4997,-0.0002]\t\t-7.3527\t\t0.1763\n",
      "Tiempo transcurrido: 2.990007\n",
      "Iteraciones: 13\n"
     ]
    }
   ],
   "source": [
    "start_time = time() #Conteo para tiempo\n",
    "iteraciones = 1 #Conteo de numero de iteraciones\n",
    "I = np.identity(2) #Se establece la matriz identidad 2x2\n",
    "labda = 10000 #Se define un labda inicial\n",
    "x = [-1,1] #Punto de partida\n",
    "x_prev = x #Definimos una variable de punto anterior\n",
    "fx_prev=f(x) #Se evalua la funcion con el punto inicial\n",
    "print('Valor de la funciÃ³n inicial = %f ' % fx_prev)\n",
    "for i in range(1000): #Proceso iterativo del algoritmo\n",
    "    xAntLM = x #Almacenamos el punto anterior\n",
    "    G = gradient(x,delta) #Se calcula el vector gradiente\n",
    "    H = Hessian(x,delta) #Se calcula la matriz Hessiana\n",
    "    S = -np.matmul(np.linalg.inv((H+labda*I)),G) #Se realiza la multiplicacion del resultado la inversa Hessiana summada la matriz Identidad multiplicada por labda, el resultado es multiplicado por el gradiente de la funcion\n",
    "    x=x+S #Se actuliza el nuevo punto optimo\n",
    "    fx=f(x) #Se evalua la funcion con el nuevo punto optimo\n",
    "    if(fx<fx_prev): #Si fx es menor al previo la nueva labda sera multiplicada por 0.5 y se tendra un nuevo tamaÃ±o de paso\n",
    "        labda=.5*labda\n",
    "    else: #Caso contrario labda sera multiplicado por 2\n",
    "        labda=2*labda\n",
    "    #Almacenamos los puntos de direccion para trazar la grafoica\n",
    "    puntosM.append([[x[0],xAntLM[0]],[x[1],xAntLM[1]]])  \n",
    "    #Establecemos como condicion de paro, que la diferencia fx_anterior-fx_actual sea menor a epsilon o que la norma del gradiente sea menor a epsilon\n",
    "    if(abs(fx_prev - fx) < ep1 or LA.norm(G)<ep1):\n",
    "        print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, x[0], x[1],fx_prev,LA.norm(G)))\n",
    "        break\n",
    "    print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, x[0], x[1],fx_prev,LA.norm(G)))\n",
    "    fx_prev=fx #Ahora fx sera fx anterior     \n",
    "    iteraciones+=1\n",
    "elapsed_time = (time() - start_time)*1000\n",
    "print(\"Tiempo transcurrido: %f\" % elapsed_time)\n",
    "print(\"Iteraciones: %d\" % iteraciones)\n",
    "#Graficamos el resultado del Steepest descent method\n",
    "graficaUnaaUna(puntosM, 'Marquardtâ€™s method') \n",
    "times.append(elapsed_time)\n",
    "iters.append(iteraciones)\n",
    "pmins.append(f(x))\n",
    "xoptm.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la funciÃ³n inicial = 270.294169 \n",
      "0\t[0.030,0.199]\t-1.026\t33.677\n",
      "1\t[0.438,0.101]\t-6.301\t39.919\n",
      "2\t[0.502,-0.001]\t-7.353\t26.034\n",
      "3\t[0.502,-0.001]\t-7.353\t0.224\n",
      "Tiempo transcurrido: 2.990007\n",
      "Iteraciones: 4\n"
     ]
    }
   ],
   "source": [
    "start_time = time() #Conteo para tiempo\n",
    "iteraciones = 1 #Conteo de numero de iteraciones\n",
    "x = [-1,1] #Punto de partida\n",
    "x_prev1 = x #Definimos una variable de punto anterior\n",
    "x_prev = x #Definimos una variable de punto anterior\n",
    "f_prev = f(x) #Se evalua la funcion con el punto inicial\n",
    "print('Valor de la funciÃ³n inicial = %f ' % f_prev)\n",
    "dire_prev=gradient(x,delta)\n",
    "si_prev = -dire_prev\n",
    "#x,search,xi,eps\n",
    "alpha, fx_prev = golden(x,si_prev,x_prev,ep1);\n",
    "x = x +  alpha*si_prev \n",
    "for i in range(1000): #Proceso iterativo del algoritmo\n",
    "    dire=gradient(x,delta)\n",
    "    si = - dire +((LA.norm(dire)**2)/(LA.norm(dire_prev)**2))*si_prev;\n",
    "    alpha, fx_curr = golden(x,si,x_prev,ep1);\n",
    "    if abs(fx_curr-fx_prev)<ep1 or LA.norm(dire)<ep1:\n",
    "        print('{0}\\t[{1:.3f},{2:.3f}]\\t{3:.3f}\\t{4:.3f}'.format(i, x[0], x[1],fx_curr,LA.norm(dire)))\n",
    "        break;\n",
    "    dire_prev=dire\n",
    "    si_prev=si\n",
    "    x = x +  (alpha*si).transpose() \n",
    "    fx_prev=f(x)\n",
    "    print('{0}\\t[{1:.3f},{2:.3f}]\\t{3:.3f}\\t{4:.3f}'.format(i, x[0], x[1],fx_curr,LA.norm(dire)))\n",
    "    puntosGC.append([[x_prev1[0],x[0]],[x_prev1[1],x[1]]])\n",
    "    x_prev1 = x\n",
    "    iteraciones+=1\n",
    "elapsed_time = (time() - start_time)*1000\n",
    "print(\"Tiempo transcurrido: %f\" % elapsed_time)\n",
    "print(\"Iteraciones: %d\" % iteraciones)\n",
    "#Graficamos el resultado del Steepest descent method\n",
    "graficaUnaaUna(puntosGC, 'Conjugate gradient method')  \n",
    "times.append(elapsed_time)\n",
    "iters.append(iteraciones)\n",
    "pmins.append(f(x))\n",
    "xoptm.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFP method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la funciÃ³n inicial = 270.294169 \n",
      "0\t[-1.000,1.000]\t-0.864\t444.316\n",
      "1\t[-0.389,-0.009]\t-4.201\t33.677\n",
      "2\t[0.022,0.199]\t-6.867\t40.221\n",
      "3\t[0.200,0.168]\t-7.267\t30.562\n",
      "4\t[0.364,0.068]\t-7.349\t7.573\n",
      "5\t[0.475,0.031]\t-7.352\t6.891\n",
      "6\t[0.490,0.007]\t-7.352\t0.861\n",
      "Tiempo transcurrido: 3.985643\n",
      "Iteraciones: 7\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "xi = [-1,1]\n",
    "x = xi\n",
    "Bi = np.eye(len(x))\n",
    "fx_prev = f(x)\n",
    "print('Valor de la funciÃ³n inicial = %f ' % fx_prev) #Valor inicial de la funcion con el punto inicial\n",
    "iteraciones = 1 #Conteo de numero de iteraciones\n",
    "Gx = gradient(x,delta) #Direccion de busqueda\n",
    "S = -Gx \n",
    "alpha,fx_prev = golden(x,S,xi,ep1)\n",
    "xi_1 = x + alpha*S       \n",
    "for j in range(1000):\n",
    "    deltax=xi_1-x\n",
    "    Gxi=gradient(xi_1,delta)\n",
    "    Gi = Gxi-Gx\n",
    "    Bi = Bi + (np.matmul(np.reshape(deltax,(2,1)),np.reshape(deltax,(1,2)))/np.matmul(deltax,Gi.transpose())) - (np.matmul(np.matmul(np.matmul(Bi, np.reshape(Gi,(2,1))), np.reshape(Gi,(1,2))),Bi) / np.matmul(np.matmul(np.reshape(Gi,(1,2)),Bi), np.reshape(Gi,(2,1))))\n",
    "    si = np.matmul(-Bi,Gxi.transpose())\n",
    "    si = np.ndarray.flatten(si.transpose())\n",
    "    alpha,fx_curr = golden(xi_1[:],si,xi,ep1)\n",
    "    print('{0}\\t[{1:.3f},{2:.3f}]\\t{3:.3f}\\t{4:.3f}'.format(j, x[0], x[1],fx_curr,LA.norm(Gx)))\n",
    "    if abs(fx_curr-fx_prev)<ep1 or LA.norm(Gx)<ep1:\n",
    "        break\n",
    "    fx_prev=fx_curr\n",
    "    Gx = Gxi\n",
    "    iteraciones+=1\n",
    "    puntosBFP.append([[x[0],xi_1[0]],[x[1],xi_1[1]]])\n",
    "    x=xi_1\n",
    "    xi_1 = x + alpha*si\n",
    "elapsed_time = (time() - start_time)*1000\n",
    "print(\"Tiempo transcurrido: %f\" % elapsed_time)\n",
    "print(\"Iteraciones: %d\" % iteraciones)\n",
    "graficaUnaaUna(puntosBFP, 'BFP')   \n",
    "times.append(elapsed_time)\n",
    "iters.append(j)\n",
    "pmins.append(f(xi_1))\n",
    "xoptm.append(xi_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270.29416855008\n",
      "0\t[1.2420,-2.7043]\t\t55.8889\t\t444.3158\n",
      "1\t[0.1960,-1.9523]\t\t74.3165\t\t443.6934\n",
      "2\t[1.1232,-1.2909]\t\t34.0478\t\t37.0109\n",
      "3\t[1.2709,-0.9555]\t\t26.8304\t\t67.5989\n",
      "4\t[0.9382,-0.1791]\t\t-0.0429\t\t90.7990\n",
      "5\t[0.8071,-0.2531]\t\t-4.1299\t\t55.0008\n",
      "6\t[0.6996,-0.1247]\t\t-6.2440\t\t16.2680\n",
      "7\t[0.5770,-0.0139]\t\t-7.1258\t\t9.5776\n",
      "8\t[0.5498,-0.0090]\t\t-7.2601\t\t9.9599\n",
      "9\t[0.5271,-0.0059]\t\t-7.3280\t\t6.2747\n",
      "10\t[0.5150,-0.0034]\t\t-7.3452\t\t3.0428\n",
      "11\t[0.5094,-0.0023]\t\t-7.3499\t\t1.6811\n",
      "12\t[0.5058,-0.0015]\t\t-7.3518\t\t1.0342\n",
      "13\t[0.5058,-0.0015]\t\t-7.3525\t\t0.6400\n",
      "Tiempo transcurrido: 6.976604\n",
      "Iteraciones: 14\n"
     ]
    }
   ],
   "source": [
    "start_time = time() #Conteo para tiempo\n",
    "iteraciones = 1 #Conteo de numero de iteraciones\n",
    "#Test the point Xi+1 for optimality. If ||âˆ‡fi+1|| â‰¤ ðœ€, where ðœ€ is a small quantity,\n",
    "#take X* â‰ˆ X i+1 and stop the process. Otherwise, go to step 5.\n",
    "#1. Start with an initial point X1\n",
    "xi = [-1,1]\n",
    "xi_1 = xi\n",
    "#n Ã— n positive definite symmetric matrix [B1] as an initial estimate of the inverse of the Hessian matrix of f\n",
    "#In the absence of additional information, [B1] is taken as the identity matrix [I].\n",
    "Bi = np.eye(len(xi))\n",
    "print(f(xi))\n",
    "fx_prev=f(xi)\n",
    "gF_xi = gradient(xi_1, delta)\n",
    "#2. Compute the gradient of the function, âˆ‡fi, at point Xi, and set\n",
    "Si = np.matmul(Bi,gF_xi)\n",
    "#3. Find the optimal step length Î»âˆ—i in the direction Si and set\n",
    "alpha, fx = golden(xi_1,Si,xi,ep1)\n",
    "xi_1 = xi - alpha*Si\n",
    "for i in range(1000): #Proceso iterativo del algoritmo\n",
    "    #Compute the gradient vector âˆ‡f1 = âˆ‡f(X1) and set the iteration number as i = 1.\n",
    "    gF_xi = gradient(xi, delta)\n",
    "    #2. Compute the gradient of the function, âˆ‡fi, at point Xi, and set\n",
    "    Si = np.matmul(np.dot(1, Bi),gF_xi)\n",
    "    #3. Find the optimal step length Î»âˆ—i in the direction Si and set\n",
    "    alpha, fx = golden(xi_1,Si,xi,ep1)\n",
    "    xi_1 = xi + np.dot(alpha, Si)\n",
    "    #4. Test the point Xi+1 for optimality. If ||âˆ‡fi+1|| â‰¤ ðœ€, where ðœ€ is a small quantity, take X* â‰ˆ X i+1 and stop the process. Otherwise, go to step 5.\n",
    "    if(LA.norm(gF_xi)<ep1 or abs(fx-fx_prev)<ep1):\n",
    "        print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, xi[0], xi[1],fx,LA.norm(gF_xi)))\n",
    "        break\n",
    "    else:\n",
    "        g1 = np.reshape(gradient(xi_1, delta),(2,1)) - np.reshape(gF_xi,(2,1))\n",
    "        d1 = np.reshape(np.array(xi),(2,1)) - np.reshape(xi_1,(2,1))\n",
    "        d1d1t = np.dot(d1, np.reshape(d1,(1,2)))\n",
    "        d1tg1 = np.matmul(d1.transpose(),g1)\n",
    "        d1g1t = np.matmul(d1, g1.transpose())\n",
    "        g1d1t = np.matmul(g1, d1.transpose())\n",
    "        g1tb1g1 = np.matmul(np.matmul(g1.transpose(), Bi), g1)\n",
    "        d1g1tb1 = np.matmul(d1g1t,Bi)\n",
    "        b1g1d1t = np.matmul(Bi, g1d1t)\n",
    "        Bi = Bi + ((1+(g1tb1g1/d1tg1))*(d1d1t/d1tg1)) - (d1g1tb1/d1tg1) - (b1g1d1t/d1tg1) \n",
    "    puntosBFGS.append([[xi_1[0],xi[0]],[xi_1[1],xi[1]]])\n",
    "    xi = xi_1\n",
    "    fx_prev = fx\n",
    "    iteraciones += 1\n",
    "    print('{0}\\t[{1:.4f},{2:.4f}]\\t\\t{3:.4f}\\t\\t{4:.4f}'.format(i, xi[0], xi[1],fx,LA.norm(gF_xi)))\n",
    "elapsed_time = (time() - start_time)*1000\n",
    "print(\"Tiempo transcurrido: %f\" % elapsed_time)\n",
    "print(\"Iteraciones: %d\" % iteraciones)\n",
    "#Graficamos el resultado del Steepest descent method\n",
    "graficaUnaaUna(puntosBFGS, 'BFGS method')    \n",
    "times.append(elapsed_time)\n",
    "iters.append(iteraciones)\n",
    "pmins.append(f(xi))\n",
    "xoptm.append(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ComparaciÃ³n de mÃ©todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparacionMetodos(puntosMetodos, nombresMetodos):\n",
    "    x = np.linspace(-1, 1.3, 100)\n",
    "    y = np.linspace(-3, 1, 100)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = f([xx, yy])\n",
    "    plt.contour(x, y, z, 20)\n",
    "    plt.plot(-1, 1, 'ro--',  markersize=6)\n",
    "    plt.plot(0.5, 0, 'go--', markersize=6)\n",
    "    i = 0    \n",
    "    for metodo in puntosMetodos:\n",
    "        #Generamos valores ramdom para generar un color diferente de grafica\n",
    "        r = random.random()\n",
    "        b = random.random()\n",
    "        g = random.random()\n",
    "        color = (r, g, b)\n",
    "        aux = 0\n",
    "        #Graficamos los trazos (direcciones) que va generando el algoritmo\n",
    "        for pts in metodo:\n",
    "            if(aux==len(metodo)-1):\n",
    "                plt.plot(pts[0],pts[1], c=color, linewidth=2, label=str(nombresMetodos[i]))\n",
    "            else:\n",
    "                plt.plot(pts[0],pts[1], c=color, linewidth=2)\n",
    "            aux = aux + 1\n",
    "        i += 1\n",
    "    plt.legend()    \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = ['Force Brute','Steepest descent method','Modified Newtonâ€™s method','Marquardtâ€™s method','Conjugate gradient method', 'BFP method','BFGS method']\n",
    "listaPuntos = [puntosFB, puntosSD, puntosN, puntosM, puntosGC, puntosBFP, puntosBFGS]\n",
    "comparacionMetodos(listaPuntos, nombres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estadisticas(metrica, nombres, ctrol):\n",
    "    y = metrica\n",
    "    z = np.arange(0,len(metrica),1)\n",
    "    n = nombres\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(z, y)\n",
    "    ax.annotate(n[0], (z[0], y[0]), xytext=(z[0]+0.4, y[0]+0.3), \n",
    "        arrowprops=dict(facecolor='red', shrink=0.05))\n",
    "    ax.annotate(n[1], (z[1], y[1]), xytext=(z[1]-0.4, y[1]-0.3), \n",
    "        arrowprops = dict(  arrowstyle=\"->\",\n",
    "                            connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "    ax.annotate(n[2], (z[2], y[2]), xytext=(z[2]-0.4, y[2]-0.3), \n",
    "        arrowprops = dict(arrowstyle=\"wedge,tail_width=0.5\", alpha=0.1))\n",
    "    ax.annotate(n[3], (z[3], y[3]), xytext=(z[3]+0.4, y[3]-0.2), \n",
    "        arrowprops = dict(arrowstyle=\"fancy\"))\n",
    "    ax.annotate(n[4], (z[4], y[4]), xytext=(z[4]-0.1, y[4]-0.2),\n",
    "        bbox=dict(boxstyle=\"round\", alpha=0.1), \n",
    "        arrowprops = dict(arrowstyle=\"simple\"))\n",
    "    ax.annotate(n[5], (z[5], y[5]), xytext=(z[5]+0.4, y[5]-0.2), \n",
    "        arrowprops = dict(arrowstyle=\"fancy\"))\n",
    "    if(ctrol==1):\n",
    "        plt.xlabel(\"MÃ©todo\")\n",
    "        plt.ylabel(\"Tiempo (s)\")\n",
    "    else:\n",
    "        plt.xlabel(\"MÃ©todo\")\n",
    "        plt.ylabel(\"Iteraciones\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.3406212644024365, -7.351450710528228, -7.3514921847513435, -7.352937586742434, -7.352742484122075, -7.352054497609378, -7.351785791967479]\n"
     ]
    }
   ],
   "source": [
    "estadisticas(times, nombres, 1)\n",
    "estadisticas(iters, nombres, 0)\n",
    "print(pmins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f89ea19deeaaf1f176387edbb9b7cc72d721332c28040f46aae8421e811c1cf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
